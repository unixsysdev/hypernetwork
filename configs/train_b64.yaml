# B=64 — Maximum Throughput / Experimental
#
# Best for: When you've validated convergence at B=16/32 and want speed
# Activation VRAM: ~352GB (fits in 8xH200, ~600GB headroom)
# Optimizer steps: 18.7K (40 epochs × 468 steps)
# Estimated runtime on 8xH200: ~1-2 hours
#
# LR uses sqrt scaling: 1e-4 × sqrt(64/8) ≈ 2.8e-4
# 40 epochs to get enough optimizer steps for convergence.
# Warmup extended to 4 epochs. Monitor val loss closely —
# large-batch distillation can underfit if LR is too conservative
# or overfit if too aggressive. Tune based on B=16 baseline.
#
# WARNING: If val loss plateaus early, this batch size may be
# too large for the dataset size (~30K samples). Fall back to B=32.

# Models
teacher:
  model_id: "Qwen/Qwen3-Coder-480B-A35B-Instruct"
  quantization: "fp8"
  tensor_parallel_size: 4
  gpu_memory_utilization: 0.90
  devices: [0, 1, 2, 3]

student:
  model_id: "Qwen/Qwen3-Coder-Next"
  dtype: "bfloat16"
  devices: [0, 1, 2, 3, 4, 5, 6, 7]
  freeze_base: true

# Hypernetwork
hypernetwork:
  hidden_dim: 2048
  num_layers: 4
  num_heads: 8
  lora_rank: 16
  lora_alpha: 32
  dropout: 0.1
  zero_init: true

# Data
data:
  dataset_id: "nebius/SWE-rebench-openhands-trajectories"
  filter_resolved: true
  max_prompt_tokens: 512
  max_trajectory_tokens: 8192
  cache_dir: "./data/cache"
  arrow_path: "./data/tokenized.arrow"

# Training
training:
  warmup_lr: 3.0e-5
  warmup_epochs: 4  # Longer warmup for larger LR
  learning_rate: 3.0e-4  # Sqrt scaled: 1e-4 × sqrt(64/8) ≈ 2.8e-4, rounded up
  epochs: 40  # More passes; 468 steps/epoch × 40 = 18.7K total steps
  batch_size: 64
  gradient_accumulation_steps: 1
  effective_batch_size: 64
  optimizer: "adamw"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  max_grad_norm: 1.0
  lr_scheduler: "cosine"
  top_k_logits: 128
  temperature: 1.0

# Validation
validation:
  val_split: 0.05
  val_every_epochs: 1

# Checkpointing
checkpointing:
  save_dir: "./checkpoints/b64"
  save_every_epochs: 4
  keep_last_n: 5

# Logging
logging:
  wandb_project: "hypernetwork-distillation"
  wandb_run_name: "b64-max-throughput"
  wandb_enabled: true
  log_every_steps: 3  # Very few steps/epoch, log often
