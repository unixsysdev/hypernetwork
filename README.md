# Hypernetwork-Based Knowledge Distillation

Distill Qwen3-Coder-480B into Qwen3-Coder-Next (80B MoE) using a Hypernetwork that generates dynamic LoRA adapters.

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run pre-flight tests (CRITICAL - do this first!)
python tests/test_gradient_flow.py

# 3. Download and prepare dataset
python scripts/download_data.py --output_dir ./data

# 4. Start training
python scripts/train.py --config configs/train_config.yaml
```

## Architecture Overview

```
┌─────────────────┐     Prompt      ┌─────────────────┐
│                 │  Embeddings     │                 │
│     Student     │ ────────────────│   Hypernetwork  │
│  (Qwen3-Next)   │                 │                 │
│                 │     LoRA        │                 │
│    (frozen)     │ ◄───────────────│   (trainable)   │
└────────┬────────┘    Weights      └─────────────────┘
         │
         │ Forward Pass (with LoRA)
         ▼
    Student Logits
         │
         │ KL Divergence
         │
         ▼
    Teacher Logits ◄─── Teacher (Qwen3-480B, frozen, FP8)
```

## Key Components

### 1. Hypernetwork (`src/hypernetwork.py`)
Generates LoRA adapters based on the input prompt:
- `PromptEncoder`: Compresses prompt into context vector
- `LoRAGenerator`: Generates A/B matrices for each layer
- Targets BOTH Attention (25%) AND DeltaNet (75%) layers

### 2. LoRA Injection (`src/lora_injection.py`)
Dynamically applies LoRA to frozen Student model:
- Context manager for clean injection/removal
- Functional interface for efficient training
- Hook-based alternative for complex models

### 3. Training Loop (`src/training.py`)
End-to-end distillation with:
- Top-K KL divergence (bandwidth efficient)
- Gradient accumulation
- WandB logging
- Checkpointing

### 4. Cluster Config (`src/cluster_config.py`)
8x H200 GPU configuration:
- GPUs 0-3: Teacher (vLLM, FP8, TP=4)
- GPUs 4-7: Student + Hypernetwork (FSDP, BF16)

## Dataset

Uses `nebius/SWE-rebench-openhands-trajectories`:
- 67,074 total trajectories
- ~32,000 resolved (gold set)
- Generated by Qwen3-Coder-480B with OpenHands agent

## Training Timeline

| Phase | Duration | Description |
|-------|----------|-------------|
| Warmup | 4 hours | 1k samples, LR=1e-5 |
| Main | 32 hours | Full data, 20 epochs |
| Annealing | 8 hours | Extended context (32k) |

## Pre-flight Checks

**CRITICAL**: Run these tests before training:

```bash
python tests/test_gradient_flow.py
```

This verifies:
- ✓ Gradients reach the Hypernetwork
- ✓ Zero-init preserves Student behavior
- ✓ Gradient magnitudes are reasonable

## License

MIT
